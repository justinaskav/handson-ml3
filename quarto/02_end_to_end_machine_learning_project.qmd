**Chapter 2 – End-to-end Machine Learning project**

*This notebook contains all the sample code and solutions to the exercises in chapter 2.*

<table align="left">
  <td>
    <a href="https://colab.research.google.com/github/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
  <td>
    <a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a>
  </td>
</table>


```python
print("Welcome to Machine Learning!")
```

    Welcome to Machine Learning!


This project requires Python 3.7 or above:


```python
import sys

assert sys.version_info >= (3, 7)
```

It also requires Scikit-Learn ≥ 1.0.1:


```python
from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")
```

# Get the Data

*Welcome to Machine Learning Housing Corp.! Your task is to predict median house values in Californian districts, given a number of features from these districts.*

## Download the Data


```python
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()
```

## Take a Quick Look at the Data Structure


```python
housing.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-122.23</td>
      <td>37.88</td>
      <td>41.0</td>
      <td>880.0</td>
      <td>129.0</td>
      <td>322.0</td>
      <td>126.0</td>
      <td>8.3252</td>
      <td>452600.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-122.22</td>
      <td>37.86</td>
      <td>21.0</td>
      <td>7099.0</td>
      <td>1106.0</td>
      <td>2401.0</td>
      <td>1138.0</td>
      <td>8.3014</td>
      <td>358500.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-122.24</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1467.0</td>
      <td>190.0</td>
      <td>496.0</td>
      <td>177.0</td>
      <td>7.2574</td>
      <td>352100.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1274.0</td>
      <td>235.0</td>
      <td>558.0</td>
      <td>219.0</td>
      <td>5.6431</td>
      <td>341300.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1627.0</td>
      <td>280.0</td>
      <td>565.0</td>
      <td>259.0</td>
      <td>3.8462</td>
      <td>342200.0</td>
      <td>NEAR BAY</td>
    </tr>
  </tbody>
</table>
</div>




```python
housing.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 20640 entries, 0 to 20639
    Data columns (total 10 columns):
     #   Column              Non-Null Count  Dtype  
    ---  ------              --------------  -----  
     0   longitude           20640 non-null  float64
     1   latitude            20640 non-null  float64
     2   housing_median_age  20640 non-null  float64
     3   total_rooms         20640 non-null  float64
     4   total_bedrooms      20433 non-null  float64
     5   population          20640 non-null  float64
     6   households          20640 non-null  float64
     7   median_income       20640 non-null  float64
     8   median_house_value  20640 non-null  float64
     9   ocean_proximity     20640 non-null  object 
    dtypes: float64(9), object(1)
    memory usage: 1.6+ MB



```python
housing["ocean_proximity"].value_counts()
```




    <1H OCEAN     9136
    INLAND        6551
    NEAR OCEAN    2658
    NEAR BAY      2290
    ISLAND           5
    Name: ocean_proximity, dtype: int64




```python
housing.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20433.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-119.569704</td>
      <td>35.631861</td>
      <td>28.639486</td>
      <td>2635.763081</td>
      <td>537.870553</td>
      <td>1425.476744</td>
      <td>499.539680</td>
      <td>3.870671</td>
      <td>206855.816909</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.003532</td>
      <td>2.135952</td>
      <td>12.585558</td>
      <td>2181.615252</td>
      <td>421.385070</td>
      <td>1132.462122</td>
      <td>382.329753</td>
      <td>1.899822</td>
      <td>115395.615874</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-124.350000</td>
      <td>32.540000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>0.499900</td>
      <td>14999.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-121.800000</td>
      <td>33.930000</td>
      <td>18.000000</td>
      <td>1447.750000</td>
      <td>296.000000</td>
      <td>787.000000</td>
      <td>280.000000</td>
      <td>2.563400</td>
      <td>119600.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>-118.490000</td>
      <td>34.260000</td>
      <td>29.000000</td>
      <td>2127.000000</td>
      <td>435.000000</td>
      <td>1166.000000</td>
      <td>409.000000</td>
      <td>3.534800</td>
      <td>179700.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>-118.010000</td>
      <td>37.710000</td>
      <td>37.000000</td>
      <td>3148.000000</td>
      <td>647.000000</td>
      <td>1725.000000</td>
      <td>605.000000</td>
      <td>4.743250</td>
      <td>264725.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>-114.310000</td>
      <td>41.950000</td>
      <td>52.000000</td>
      <td>39320.000000</td>
      <td>6445.000000</td>
      <td>35682.000000</td>
      <td>6082.000000</td>
      <td>15.000100</td>
      <td>500001.000000</td>
    </tr>
  </tbody>
</table>
</div>



The following cell is not shown either in the book. It creates the `images/end_to_end_project` folder (if it doesn't already exist), and it defines the `save_fig()` function which is used through this notebook to save the figures in high-res for the book.


```python
# extra code – code to save the figures as high-res PNGs for the book

IMAGES_PATH = Path() / "images" / "end_to_end_project"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```


```python
import matplotlib.pyplot as plt

# extra code – the next 5 lines define the default font sizes
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

housing.hist(bins=50, figsize=(12, 8))
save_fig("attribute_histogram_plots")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_19_0.png)
    


## Create a Test Set


```python
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```


```python
train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)
```




    16512




```python
len(test_set)
```




    4128



To ensure that this notebook's outputs remain the same every time we run it, we need to set the random seed:


```python
np.random.seed(42)
```

Sadly, this won't guarantee that this notebook will output exactly the same results as in the book, since there are other possible sources of variation. The most important is the fact that algorithms get tweaked over time when libraries evolve. So please tolerate some minor differences: hopefully, most of the outputs should be the same, or at least in the right ballpark.

Note: another source of randomness is the order of Python sets: it is based on Python's `hash()` function, which is randomly "salted" when Python starts up (this started in Python 3.3, to prevent some denial-of-service attacks). To remove this randomness, the solution is to set the `PYTHONHASHSEED` environment variable to `"0"` _before_ Python even starts up. Nothing will happen if you do it after that. Luckily, if you're running this notebook on Colab, the variable is already set for you.


```python
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```


```python
housing_with_id = housing.reset_index()  # adds an `index` column
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")
```


```python
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")
```


```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```


```python
test_set["total_bedrooms"].isnull().sum()
```




    44



To find the probability that a random sample of 1,000 people contains less than 48.5% female or more than 53.5% female when the population's female ratio is 51.1%, we use the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution). The `cdf()` method of the binomial distribution gives us the probability that the number of females will be equal or less than the given value.


```python
# extra code – shows how to compute the 10.7% proba of getting a bad sample

from scipy.stats import binom

sample_size = 1000
ratio_female = 0.511
proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)
proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)
print(proba_too_small + proba_too_large)
```

    0.10736798530929946


If you prefer simulations over maths, here's how you could get roughly the same result:


```python
# extra code – shows another way to estimate the probability of bad sample

np.random.seed(42)

samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)
((samples < 485) | (samples > 535)).mean()
```




    0.1071




```python
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
```


```python
housing["income_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.xlabel("Income category")
plt.ylabel("Number of districts")
save_fig("housing_income_cat_bar_plot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_38_0.png)
    



```python
from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []
for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])
```


```python
strat_train_set, strat_test_set = strat_splits[0]
```

It's much shorter to get a single stratified split:


```python
strat_train_set, strat_test_set = train_test_split(
    housing, test_size=0.2, stratify=housing["income_cat"], random_state=42)
```


```python
strat_test_set["income_cat"].value_counts() / len(strat_test_set)
```




    3    0.350533
    2    0.318798
    4    0.176357
    5    0.114341
    1    0.039971
    Name: income_cat, dtype: float64




```python
# extra code – computes the data for Figure 2–10

def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

compare_props = pd.DataFrame({
    "Overall %": income_cat_proportions(housing),
    "Stratified %": income_cat_proportions(strat_test_set),
    "Random %": income_cat_proportions(test_set),
}).sort_index()
compare_props.index.name = "Income Category"
compare_props["Strat. Error %"] = (compare_props["Stratified %"] /
                                   compare_props["Overall %"] - 1)
compare_props["Rand. Error %"] = (compare_props["Random %"] /
                                  compare_props["Overall %"] - 1)
(compare_props * 100).round(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Overall %</th>
      <th>Stratified %</th>
      <th>Random %</th>
      <th>Strat. Error %</th>
      <th>Rand. Error %</th>
    </tr>
    <tr>
      <th>Income Category</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>3.98</td>
      <td>4.00</td>
      <td>4.24</td>
      <td>0.36</td>
      <td>6.45</td>
    </tr>
    <tr>
      <th>2</th>
      <td>31.88</td>
      <td>31.88</td>
      <td>30.74</td>
      <td>-0.02</td>
      <td>-3.59</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35.06</td>
      <td>35.05</td>
      <td>34.52</td>
      <td>-0.01</td>
      <td>-1.53</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.63</td>
      <td>17.64</td>
      <td>18.41</td>
      <td>0.03</td>
      <td>4.42</td>
    </tr>
    <tr>
      <th>5</th>
      <td>11.44</td>
      <td>11.43</td>
      <td>12.09</td>
      <td>-0.08</td>
      <td>5.63</td>
    </tr>
  </tbody>
</table>
</div>




```python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

# Discover and Visualize the Data to Gain Insights


```python
housing = strat_train_set.copy()
```

## Visualizing Geographical Data


```python
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True)
save_fig("bad_visualization_plot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_49_0.png)
    



```python
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.2)
save_fig("better_visualization_plot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_50_0.png)
    



```python
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True,
             s=housing["population"] / 100, label="population",
             c="median_house_value", cmap="jet", colorbar=True,
             legend=True, sharex=False, figsize=(10, 7))
save_fig("housing_prices_scatterplot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_51_0.png)
    


The argument `sharex=False` fixes a display bug: without it, the x-axis values and label are not displayed (see: https://github.com/pandas-dev/pandas/issues/10611).

The next cell generates the first figure in the chapter (this code is not in the book). It's just a beautified version of the previous figure, with an image of California added in the background, nicer label names and no grid.


```python
# extra code – this cell generates the first figure in the chapter

# Download the California image
filename = "california.png"
if not (IMAGES_PATH / filename).is_file():
    homl3_root = "https://github.com/ageron/handson-ml3/raw/main/"
    url = homl3_root + "images/end_to_end_project/" + filename
    print("Downloading", filename)
    urllib.request.urlretrieve(url, IMAGES_PATH / filename)

housing_renamed = housing.rename(columns={
    "latitude": "Latitude", "longitude": "Longitude",
    "population": "Population",
    "median_house_value": "Median house value (ᴜsᴅ)"})
housing_renamed.plot(
             kind="scatter", x="Longitude", y="Latitude",
             s=housing_renamed["Population"] / 100, label="Population",
             c="Median house value (ᴜsᴅ)", cmap="jet", colorbar=True,
             legend=True, sharex=False, figsize=(10, 7))

california_img = plt.imread(IMAGES_PATH / filename)
axis = -124.55, -113.95, 32.45, 42.05
plt.axis(axis)
plt.imshow(california_img, extent=axis)

save_fig("california_housing_prices_plot")
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_54_0.png)
    


## Looking for Correlations

Note: since Pandas 2.0.0, the `numeric_only` argument defaults to `False`, so we need to set it explicitly to True to avoid an error.


```python
corr_matrix = housing.corr(numeric_only=True)
```


```python
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value    1.000000
    median_income         0.688380
    total_rooms           0.137455
    housing_median_age    0.102175
    households            0.071426
    total_bedrooms        0.054635
    population           -0.020153
    longitude            -0.050859
    latitude             -0.139584
    Name: median_house_value, dtype: float64




```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
save_fig("scatter_matrix_plot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_59_0.png)
    



```python
housing.plot(kind="scatter", x="median_income", y="median_house_value",
             alpha=0.1, grid=True)
save_fig("income_vs_house_value_scatterplot")  # extra code
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_60_0.png)
    


## Experimenting with Attribute Combinations


```python
housing["rooms_per_house"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_ratio"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["people_per_house"] = housing["population"] / housing["households"]
```


```python
corr_matrix = housing.corr(numeric_only=True)
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value    1.000000
    median_income         0.688380
    rooms_per_house       0.143663
    total_rooms           0.137455
    housing_median_age    0.102175
    households            0.071426
    total_bedrooms        0.054635
    population           -0.020153
    people_per_house     -0.038224
    longitude            -0.050859
    latitude             -0.139584
    bedrooms_ratio       -0.256397
    Name: median_house_value, dtype: float64



# Prepare the Data for Machine Learning Algorithms

Let's revert to the original training set and separate the target (note that `strat_train_set.drop()` creates a copy of `strat_train_set` without the column, it doesn't actually modify `strat_train_set` itself, unless you pass `inplace=True`):


```python
housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"].copy()
```

## Data Cleaning

In the book 3 options are listed to handle the NaN values:

```python
housing.dropna(subset=["total_bedrooms"], inplace=True)    # option 1

housing.drop("total_bedrooms", axis=1)       # option 2

median = housing["total_bedrooms"].median()  # option 3
housing["total_bedrooms"].fillna(median, inplace=True)
```

For each option, we'll create a copy of `housing` and work on that copy to avoid breaking `housing`. We'll also show the output of each option, but filtering on the rows that originally contained a NaN value.


```python
null_rows_idx = housing.isnull().any(axis=1)
housing.loc[null_rows_idx].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14452</th>
      <td>-120.67</td>
      <td>40.50</td>
      <td>15.0</td>
      <td>5343.0</td>
      <td>NaN</td>
      <td>2503.0</td>
      <td>902.0</td>
      <td>3.5962</td>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>18217</th>
      <td>-117.96</td>
      <td>34.03</td>
      <td>35.0</td>
      <td>2093.0</td>
      <td>NaN</td>
      <td>1755.0</td>
      <td>403.0</td>
      <td>3.4115</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>11889</th>
      <td>-118.05</td>
      <td>34.04</td>
      <td>33.0</td>
      <td>1348.0</td>
      <td>NaN</td>
      <td>1098.0</td>
      <td>257.0</td>
      <td>4.2917</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>20325</th>
      <td>-118.88</td>
      <td>34.17</td>
      <td>15.0</td>
      <td>4260.0</td>
      <td>NaN</td>
      <td>1701.0</td>
      <td>669.0</td>
      <td>5.1033</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>14360</th>
      <td>-117.87</td>
      <td>33.62</td>
      <td>8.0</td>
      <td>1266.0</td>
      <td>NaN</td>
      <td>375.0</td>
      <td>183.0</td>
      <td>9.8020</td>
      <td>&lt;1H OCEAN</td>
    </tr>
  </tbody>
</table>
</div>




```python
housing_option1 = housing.copy()

housing_option1.dropna(subset=["total_bedrooms"], inplace=True)  # option 1

housing_option1.loc[null_rows_idx].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>




```python
housing_option2 = housing.copy()

housing_option2.drop("total_bedrooms", axis=1, inplace=True)  # option 2

housing_option2.loc[null_rows_idx].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14452</th>
      <td>-120.67</td>
      <td>40.50</td>
      <td>15.0</td>
      <td>5343.0</td>
      <td>2503.0</td>
      <td>902.0</td>
      <td>3.5962</td>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>18217</th>
      <td>-117.96</td>
      <td>34.03</td>
      <td>35.0</td>
      <td>2093.0</td>
      <td>1755.0</td>
      <td>403.0</td>
      <td>3.4115</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>11889</th>
      <td>-118.05</td>
      <td>34.04</td>
      <td>33.0</td>
      <td>1348.0</td>
      <td>1098.0</td>
      <td>257.0</td>
      <td>4.2917</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>20325</th>
      <td>-118.88</td>
      <td>34.17</td>
      <td>15.0</td>
      <td>4260.0</td>
      <td>1701.0</td>
      <td>669.0</td>
      <td>5.1033</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>14360</th>
      <td>-117.87</td>
      <td>33.62</td>
      <td>8.0</td>
      <td>1266.0</td>
      <td>375.0</td>
      <td>183.0</td>
      <td>9.8020</td>
      <td>&lt;1H OCEAN</td>
    </tr>
  </tbody>
</table>
</div>




```python
housing_option3 = housing.copy()

median = housing["total_bedrooms"].median()
housing_option3["total_bedrooms"].fillna(median, inplace=True)  # option 3

housing_option3.loc[null_rows_idx].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14452</th>
      <td>-120.67</td>
      <td>40.50</td>
      <td>15.0</td>
      <td>5343.0</td>
      <td>434.0</td>
      <td>2503.0</td>
      <td>902.0</td>
      <td>3.5962</td>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>18217</th>
      <td>-117.96</td>
      <td>34.03</td>
      <td>35.0</td>
      <td>2093.0</td>
      <td>434.0</td>
      <td>1755.0</td>
      <td>403.0</td>
      <td>3.4115</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>11889</th>
      <td>-118.05</td>
      <td>34.04</td>
      <td>33.0</td>
      <td>1348.0</td>
      <td>434.0</td>
      <td>1098.0</td>
      <td>257.0</td>
      <td>4.2917</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>20325</th>
      <td>-118.88</td>
      <td>34.17</td>
      <td>15.0</td>
      <td>4260.0</td>
      <td>434.0</td>
      <td>1701.0</td>
      <td>669.0</td>
      <td>5.1033</td>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>14360</th>
      <td>-117.87</td>
      <td>33.62</td>
      <td>8.0</td>
      <td>1266.0</td>
      <td>434.0</td>
      <td>375.0</td>
      <td>183.0</td>
      <td>9.8020</td>
      <td>&lt;1H OCEAN</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
```

Separating out the numerical attributes to use the `"median"` strategy (as it cannot be calculated on text attributes like `ocean_proximity`):


```python
housing_num = housing.select_dtypes(include=[np.number])
```


```python
imputer.fit(housing_num)
```




    SimpleImputer(strategy='median')




```python
imputer.statistics_
```




    array([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,
            408.    ,    3.5385])



Check that this is the same as manually computing the median of each attribute:


```python
housing_num.median().values
```




    array([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,
            408.    ,    3.5385])



Transform the training set:


```python
X = imputer.transform(housing_num)
```


```python
imputer.feature_names_in_
```




    array(['longitude', 'latitude', 'housing_median_age', 'total_rooms',
           'total_bedrooms', 'population', 'households', 'median_income'],
          dtype=object)




```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing_num.index)
```


```python
housing_tr.loc[null_rows_idx].head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14452</th>
      <td>-120.67</td>
      <td>40.50</td>
      <td>15.0</td>
      <td>5343.0</td>
      <td>434.0</td>
      <td>2503.0</td>
      <td>902.0</td>
      <td>3.5962</td>
    </tr>
    <tr>
      <th>18217</th>
      <td>-117.96</td>
      <td>34.03</td>
      <td>35.0</td>
      <td>2093.0</td>
      <td>434.0</td>
      <td>1755.0</td>
      <td>403.0</td>
      <td>3.4115</td>
    </tr>
    <tr>
      <th>11889</th>
      <td>-118.05</td>
      <td>34.04</td>
      <td>33.0</td>
      <td>1348.0</td>
      <td>434.0</td>
      <td>1098.0</td>
      <td>257.0</td>
      <td>4.2917</td>
    </tr>
    <tr>
      <th>20325</th>
      <td>-118.88</td>
      <td>34.17</td>
      <td>15.0</td>
      <td>4260.0</td>
      <td>434.0</td>
      <td>1701.0</td>
      <td>669.0</td>
      <td>5.1033</td>
    </tr>
    <tr>
      <th>14360</th>
      <td>-117.87</td>
      <td>33.62</td>
      <td>8.0</td>
      <td>1266.0</td>
      <td>434.0</td>
      <td>375.0</td>
      <td>183.0</td>
      <td>9.8020</td>
    </tr>
  </tbody>
</table>
</div>




```python
imputer.strategy
```




    'median'




```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing_num.index)
```


```python
housing_tr.loc[null_rows_idx].head()  # not shown in the book
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14452</th>
      <td>-120.67</td>
      <td>40.50</td>
      <td>15.0</td>
      <td>5343.0</td>
      <td>434.0</td>
      <td>2503.0</td>
      <td>902.0</td>
      <td>3.5962</td>
    </tr>
    <tr>
      <th>18217</th>
      <td>-117.96</td>
      <td>34.03</td>
      <td>35.0</td>
      <td>2093.0</td>
      <td>434.0</td>
      <td>1755.0</td>
      <td>403.0</td>
      <td>3.4115</td>
    </tr>
    <tr>
      <th>11889</th>
      <td>-118.05</td>
      <td>34.04</td>
      <td>33.0</td>
      <td>1348.0</td>
      <td>434.0</td>
      <td>1098.0</td>
      <td>257.0</td>
      <td>4.2917</td>
    </tr>
    <tr>
      <th>20325</th>
      <td>-118.88</td>
      <td>34.17</td>
      <td>15.0</td>
      <td>4260.0</td>
      <td>434.0</td>
      <td>1701.0</td>
      <td>669.0</td>
      <td>5.1033</td>
    </tr>
    <tr>
      <th>14360</th>
      <td>-117.87</td>
      <td>33.62</td>
      <td>8.0</td>
      <td>1266.0</td>
      <td>434.0</td>
      <td>375.0</td>
      <td>183.0</td>
      <td>9.8020</td>
    </tr>
  </tbody>
</table>
</div>




```python
#from sklearn import set_config
#
# set_config(transform_output="pandas")  # scikit-learn >= 1.2
```

Now let's drop some outliers:


```python
from sklearn.ensemble import IsolationForest

isolation_forest = IsolationForest(random_state=42)
outlier_pred = isolation_forest.fit_predict(X)
```


```python
outlier_pred
```




    array([-1,  1,  1, ...,  1,  1,  1])



If you wanted to drop outliers, you would run the following code:


```python
#housing = housing.iloc[outlier_pred == 1]
#housing_labels = housing_labels.iloc[outlier_pred == 1]
```

## Handling Text and Categorical Attributes

Now let's preprocess the categorical input feature, `ocean_proximity`:


```python
housing_cat = housing[["ocean_proximity"]]
housing_cat.head(8)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13096</th>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>14973</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>3785</th>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>14689</th>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>20507</th>
      <td>NEAR OCEAN</td>
    </tr>
    <tr>
      <th>1286</th>
      <td>INLAND</td>
    </tr>
    <tr>
      <th>18078</th>
      <td>&lt;1H OCEAN</td>
    </tr>
    <tr>
      <th>4396</th>
      <td>NEAR BAY</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
```


```python
housing_cat_encoded[:8]
```




    array([[3.],
           [0.],
           [1.],
           [1.],
           [4.],
           [1.],
           [0.],
           [3.]])




```python
ordinal_encoder.categories_
```




    [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
           dtype=object)]




```python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
```


```python
housing_cat_1hot
```




    <16512x5 sparse matrix of type '<class 'numpy.float64'>'
    	with 16512 stored elements in Compressed Sparse Row format>



By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:


```python
housing_cat_1hot.toarray()
```




    array([[0., 0., 0., 1., 0.],
           [1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.],
           ...,
           [0., 0., 0., 0., 1.],
           [1., 0., 0., 0., 0.],
           [0., 0., 0., 0., 1.]])



Alternatively, you can set `sparse_output=False` when creating the `OneHotEncoder` (note: the `sparse` hyperparameter was renamned to `sparse_output` in Scikit-Learn 1.2):


```python
cat_encoder = OneHotEncoder(sparse_output=False)
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot
```




    array([[0., 0., 0., 1., 0.],
           [1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.],
           ...,
           [0., 0., 0., 0., 1.],
           [1., 0., 0., 0., 0.],
           [0., 0., 0., 0., 1.]])




```python
cat_encoder.categories_
```




    [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
           dtype=object)]




```python
df_test = pd.DataFrame({"ocean_proximity": ["INLAND", "NEAR BAY"]})
pd.get_dummies(df_test)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ocean_proximity_INLAND</th>
      <th>ocean_proximity_NEAR BAY</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
cat_encoder.transform(df_test)
```




    array([[0., 1., 0., 0., 0.],
           [0., 0., 0., 1., 0.]])




```python
df_test_unknown = pd.DataFrame({"ocean_proximity": ["<2H OCEAN", "ISLAND"]})
pd.get_dummies(df_test_unknown)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ocean_proximity_&lt;2H OCEAN</th>
      <th>ocean_proximity_ISLAND</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
cat_encoder.handle_unknown = "ignore"
cat_encoder.transform(df_test_unknown)
```




    array([[0., 0., 0., 0., 0.],
           [0., 0., 1., 0., 0.]])




```python
cat_encoder.feature_names_in_
```




    array(['ocean_proximity'], dtype=object)




```python
cat_encoder.get_feature_names_out()
```




    array(['ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',
           'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',
           'ocean_proximity_NEAR OCEAN'], dtype=object)




```python
df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),
                         columns=cat_encoder.get_feature_names_out(),
                         index=df_test_unknown.index)
```


```python
df_output
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ocean_proximity_&lt;1H OCEAN</th>
      <th>ocean_proximity_INLAND</th>
      <th>ocean_proximity_ISLAND</th>
      <th>ocean_proximity_NEAR BAY</th>
      <th>ocean_proximity_NEAR OCEAN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



## Feature Scaling


```python
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
```


```python
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_num)
```


```python
# extra code – this cell generates Figure 2–17
fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)
housing["population"].hist(ax=axs[0], bins=50)
housing["population"].apply(np.log).hist(ax=axs[1], bins=50)
axs[0].set_xlabel("Population")
axs[1].set_xlabel("Log of population")
axs[0].set_ylabel("Number of districts")
save_fig("long_tail_plot")
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_118_0.png)
    


What if we replace each value with its percentile?


```python
# extra code – just shows that we get a uniform distribution
percentiles = [np.percentile(housing["median_income"], p)
               for p in range(1, 100)]
flattened_median_income = pd.cut(housing["median_income"],
                                 bins=[-np.inf] + percentiles + [np.inf],
                                 labels=range(1, 100 + 1))
flattened_median_income.hist(bins=50)
plt.xlabel("Median income percentile")
plt.ylabel("Number of districts")
plt.show()
# Note: incomes below the 1st percentile are labeled 1, and incomes above the
# 99th percentile are labeled 100. This is why the distribution below ranges
# from 1 to 100 (not 0 to 100).
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_120_0.png)
    



```python
from sklearn.metrics.pairwise import rbf_kernel

age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)
```


```python
# extra code – this cell generates Figure 2–18

ages = np.linspace(housing["housing_median_age"].min(),
                   housing["housing_median_age"].max(),
                   500).reshape(-1, 1)
gamma1 = 0.1
gamma2 = 0.03
rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)
rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)

fig, ax1 = plt.subplots()

ax1.set_xlabel("Housing median age")
ax1.set_ylabel("Number of districts")
ax1.hist(housing["housing_median_age"], bins=50)

ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis
color = "blue"
ax2.plot(ages, rbf1, color=color, label="gamma = 0.10")
ax2.plot(ages, rbf2, color=color, label="gamma = 0.03", linestyle="--")
ax2.tick_params(axis='y', labelcolor=color)
ax2.set_ylabel("Age similarity", color=color)

plt.legend(loc="upper left")
save_fig("age_similarity_plot")
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_122_0.png)
    



```python
from sklearn.linear_model import LinearRegression

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())

model = LinearRegression()
model.fit(housing[["median_income"]], scaled_labels)
some_new_data = housing[["median_income"]].iloc[:5]  # pretend this is new data

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)
```


```python
predictions
```




    array([[131997.15275877],
           [299359.35844434],
           [146023.37185694],
           [138840.33653057],
           [192016.61557639]])




```python
from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())
model.fit(housing[["median_income"]], housing_labels)
predictions = model.predict(some_new_data)
```


```python
predictions
```




    array([131997.15275877, 299359.35844434, 146023.37185694, 138840.33653057,
           192016.61557639])



## Custom Transformers

To create simple transformers:


```python
from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_pop = log_transformer.transform(housing[["population"]])
```


```python
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))
age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])
```


```python
age_simil_35
```




    array([[2.81118530e-13],
           [8.20849986e-02],
           [6.70320046e-01],
           ...,
           [9.55316054e-22],
           [6.70320046e-01],
           [3.03539138e-04]])




```python
sf_coords = 37.7749, -122.41
sf_transformer = FunctionTransformer(rbf_kernel,
                                     kw_args=dict(Y=[sf_coords], gamma=0.1))
sf_simil = sf_transformer.transform(housing[["latitude", "longitude"]])
```


```python
sf_simil
```




    array([[0.999927  ],
           [0.05258419],
           [0.94864161],
           ...,
           [0.00388525],
           [0.05038518],
           [0.99868067]])




```python
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
```




    array([[0.5 ],
           [0.75]])




```python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # no *args or **kwargs!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # y is required even though we don't use it
        X = check_array(X)  # checks that X is an array with finite float values
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
        return self  # always return self!

    def transform(self, X):
        check_is_fitted(self)  # looks for learned attributes (with trailing _)
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
```


```python
from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, n_init=10,
                              random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # always return self!

    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
```

**Warning**:
* There was a change in Scikit-Learn 1.3.0 which affected the random number generator for `KMeans` initialization. Therefore the results will be different than in the book if you use Scikit-Learn ≥ 1.3. That's not a problem as long as you don't expect the outputs to be perfectly identical.
* Throughout this notebook, when `n_init` was not set when creating a `KMeans` estimator, I explicitly set it to `n_init=10` to avoid a warning about the fact that the default value for this hyperparameter will change from 10 to `"auto"` in Scikit-Learn 1.4.


```python
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
similarities = cluster_simil.fit_transform(housing[["latitude", "longitude"]],
                                           sample_weight=housing_labels)
```


```python
similarities[:3].round(2)
```




    array([[0.  , 0.14, 0.  , 0.  , 0.  , 0.08, 0.  , 0.99, 0.  , 0.6 ],
           [0.63, 0.  , 0.99, 0.  , 0.  , 0.  , 0.04, 0.  , 0.11, 0.  ],
           [0.  , 0.29, 0.  , 0.  , 0.01, 0.44, 0.  , 0.7 , 0.  , 0.3 ]])




```python
# extra code – this cell generates Figure 2–19

housing_renamed = housing.rename(columns={
    "latitude": "Latitude", "longitude": "Longitude",
    "population": "Population",
    "median_house_value": "Median house value (ᴜsᴅ)"})
housing_renamed["Max cluster similarity"] = similarities.max(axis=1)

housing_renamed.plot(kind="scatter", x="Longitude", y="Latitude", grid=True,
                     s=housing_renamed["Population"] / 100, label="Population",
                     c="Max cluster similarity",
                     cmap="jet", colorbar=True,
                     legend=True, sharex=False, figsize=(10, 7))
plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],
         cluster_simil.kmeans_.cluster_centers_[:, 0],
         linestyle="", color="black", marker="X", markersize=20,
         label="Cluster centers")
plt.legend(loc="upper right")
save_fig("district_cluster_plot")
plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_140_0.png)
    


## Transformation Pipelines

Now let's build a pipeline to preprocess the numerical attributes:


```python
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])
```


```python
from sklearn.pipeline import make_pipeline

num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())
```


```python
from sklearn import set_config

set_config(display='diagram')

num_pipeline
```




    Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),
                    ('standardscaler', StandardScaler())])




```python
housing_num_prepared = num_pipeline.fit_transform(housing_num)
housing_num_prepared[:2].round(2)
```




    array([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],
           [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])




```python
def monkey_patch_get_signature_names_out():
    """Monkey patch some classes which did not handle get_feature_names_out()
       correctly in Scikit-Learn 1.0.*."""
    from inspect import Signature, signature, Parameter
    import pandas as pd
    from sklearn.impute import SimpleImputer
    from sklearn.pipeline import make_pipeline, Pipeline
    from sklearn.preprocessing import FunctionTransformer, StandardScaler

    default_get_feature_names_out = StandardScaler.get_feature_names_out

    if not hasattr(SimpleImputer, "get_feature_names_out"):
      print("Monkey-patching SimpleImputer.get_feature_names_out()")
      SimpleImputer.get_feature_names_out = default_get_feature_names_out

    if not hasattr(FunctionTransformer, "get_feature_names_out"):
        print("Monkey-patching FunctionTransformer.get_feature_names_out()")
        orig_init = FunctionTransformer.__init__
        orig_sig = signature(orig_init)

        def __init__(*args, feature_names_out=None, **kwargs):
            orig_sig.bind(*args, **kwargs)
            orig_init(*args, **kwargs)
            args[0].feature_names_out = feature_names_out

        __init__.__signature__ = Signature(
            list(signature(orig_init).parameters.values()) + [
                Parameter("feature_names_out", Parameter.KEYWORD_ONLY)])

        def get_feature_names_out(self, names=None):
            if callable(self.feature_names_out):
                return self.feature_names_out(self, names)
            assert self.feature_names_out == "one-to-one"
            return default_get_feature_names_out(self, names)

        FunctionTransformer.__init__ = __init__
        FunctionTransformer.get_feature_names_out = get_feature_names_out

monkey_patch_get_signature_names_out()
```


```python
df_housing_num_prepared = pd.DataFrame(
    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),
    index=housing_num.index)
```


```python
df_housing_num_prepared.head(2)  # extra code
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13096</th>
      <td>-1.423037</td>
      <td>1.013606</td>
      <td>1.861119</td>
      <td>0.311912</td>
      <td>1.368167</td>
      <td>0.137460</td>
      <td>1.394812</td>
      <td>-0.936491</td>
    </tr>
    <tr>
      <th>14973</th>
      <td>0.596394</td>
      <td>-0.702103</td>
      <td>0.907630</td>
      <td>-0.308620</td>
      <td>-0.435925</td>
      <td>-0.693771</td>
      <td>-0.373485</td>
      <td>1.171942</td>
    </tr>
  </tbody>
</table>
</div>




```python
num_pipeline.steps
```




    [('simpleimputer', SimpleImputer(strategy='median')),
     ('standardscaler', StandardScaler())]




```python
num_pipeline[1]
```




    StandardScaler()




```python
num_pipeline[:-1]
```




    Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median'))])




```python
num_pipeline.named_steps["simpleimputer"]
```




    SimpleImputer(strategy='median')




```python
num_pipeline.set_params(simpleimputer__strategy="median")
```




    Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),
                    ('standardscaler', StandardScaler())])




```python
from sklearn.compose import ColumnTransformer

num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms",
               "total_bedrooms", "population", "households", "median_income"]
cat_attribs = ["ocean_proximity"]

cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessing = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])
```


```python
from sklearn.compose import make_column_selector, make_column_transformer

preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)
```


```python
housing_prepared = preprocessing.fit_transform(housing)
```


```python
# extra code – shows that we can get a DataFrame out if we want
housing_prepared_fr = pd.DataFrame(
    housing_prepared,
    columns=preprocessing.get_feature_names_out(),
    index=housing.index)
housing_prepared_fr.head(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pipeline-1__longitude</th>
      <th>pipeline-1__latitude</th>
      <th>pipeline-1__housing_median_age</th>
      <th>pipeline-1__total_rooms</th>
      <th>pipeline-1__total_bedrooms</th>
      <th>pipeline-1__population</th>
      <th>pipeline-1__households</th>
      <th>pipeline-1__median_income</th>
      <th>pipeline-2__ocean_proximity_&lt;1H OCEAN</th>
      <th>pipeline-2__ocean_proximity_INLAND</th>
      <th>pipeline-2__ocean_proximity_ISLAND</th>
      <th>pipeline-2__ocean_proximity_NEAR BAY</th>
      <th>pipeline-2__ocean_proximity_NEAR OCEAN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13096</th>
      <td>-1.423037</td>
      <td>1.013606</td>
      <td>1.861119</td>
      <td>0.311912</td>
      <td>1.368167</td>
      <td>0.137460</td>
      <td>1.394812</td>
      <td>-0.936491</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>14973</th>
      <td>0.596394</td>
      <td>-0.702103</td>
      <td>0.907630</td>
      <td>-0.308620</td>
      <td>-0.435925</td>
      <td>-0.693771</td>
      <td>-0.373485</td>
      <td>1.171942</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]  # feature names out

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())

log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler())
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
                                     StandardScaler())
preprocessing = ColumnTransformer([
        ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
        ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
        ("people_per_house", ratio_pipeline(), ["population", "households"]),
        ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",
                               "households", "median_income"]),
        ("geo", cluster_simil, ["latitude", "longitude"]),
        ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
    ],
    remainder=default_num_pipeline)  # one column remaining: housing_median_age
```


```python
housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape
```




    (16512, 24)




```python
preprocessing.get_feature_names_out()
```




    array(['bedrooms__ratio', 'rooms_per_house__ratio',
           'people_per_house__ratio', 'log__total_bedrooms',
           'log__total_rooms', 'log__population', 'log__households',
           'log__median_income', 'geo__Cluster 0 similarity',
           'geo__Cluster 1 similarity', 'geo__Cluster 2 similarity',
           'geo__Cluster 3 similarity', 'geo__Cluster 4 similarity',
           'geo__Cluster 5 similarity', 'geo__Cluster 6 similarity',
           'geo__Cluster 7 similarity', 'geo__Cluster 8 similarity',
           'geo__Cluster 9 similarity', 'cat__ocean_proximity_<1H OCEAN',
           'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',
           'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',
           'remainder__housing_median_age'], dtype=object)



# Select and Train a Model

## Training and Evaluating on the Training Set


```python
from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)
```




    Pipeline(steps=[('columntransformer',
                     ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                  SimpleImputer(strategy='median')),
                                                                 ('standardscaler',
                                                                  StandardScaler())]),
                                       transformers=[('bedrooms',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='median')),
                                                                      ('functiontransformer',
                                                                       FunctionTransformer(feature_names_out=<function ratio_name at 0x1a5...
                                                       'households',
                                                       'median_income']),
                                                     ('geo',
                                                      ClusterSimilarity(random_state=42),
                                                      ['latitude', 'longitude']),
                                                     ('cat',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='most_frequent')),
                                                                      ('onehotencoder',
                                                                       OneHotEncoder(handle_unknown='ignore'))]),
                                                      <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                    ('linearregression', LinearRegression())])



Let's try the full preprocessing pipeline on a few training instances:


```python
housing_predictions = lin_reg.predict(housing)
housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred
```




    array([243700., 372400., 128800.,  94400., 328300.])



Compare against the actual values:


```python
housing_labels.iloc[:5].values
```




    array([458300., 483800., 101700.,  96100., 361800.])




```python
# extra code – computes the error ratios discussed in the book
error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1
print(", ".join([f"{100 * ratio:.1f}%" for ratio in error_ratios]))
```

    -46.8%, -23.0%, 26.6%, -1.8%, -9.3%



```python
from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
lin_rmse
```




    68687.89176589991




```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree_reg.fit(housing, housing_labels)
```




    Pipeline(steps=[('columntransformer',
                     ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                  SimpleImputer(strategy='median')),
                                                                 ('standardscaler',
                                                                  StandardScaler())]),
                                       transformers=[('bedrooms',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='median')),
                                                                      ('functiontransformer',
                                                                       FunctionTransformer(feature_names_out=<function ratio_name at 0x1a5...
                                                     ('geo',
                                                      ClusterSimilarity(random_state=42),
                                                      ['latitude', 'longitude']),
                                                     ('cat',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='most_frequent')),
                                                                      ('onehotencoder',
                                                                       OneHotEncoder(handle_unknown='ignore'))]),
                                                      <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                    ('decisiontreeregressor',
                     DecisionTreeRegressor(random_state=42))])




```python
housing_predictions = tree_reg.predict(housing)
tree_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
tree_rmse
```




    0.0



## Better Evaluation Using Cross-Validation


```python
from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
```


```python
pd.Series(tree_rmses).describe()
```




    count       10.000000
    mean     66868.027288
    std       2060.966425
    min      63649.536493
    25%      65338.078316
    50%      66801.953094
    75%      68229.934454
    max      70094.778246
    dtype: float64




```python
# extra code – computes the error stats for the linear model
lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
pd.Series(lin_rmses).describe()
```




    count       10.000000
    mean     69858.018195
    std       4182.205077
    min      65397.780144
    25%      68070.536263
    50%      68619.737842
    75%      69810.076342
    max      80959.348171
    dtype: float64



**Warning:** the following cell may take a few minutes to run:


```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing,
                           RandomForestRegressor(random_state=42))
forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,
                                scoring="neg_root_mean_squared_error", cv=10)
```


```python
pd.Series(forest_rmses).describe()
```




    count       10.000000
    mean     47019.561281
    std       1033.957120
    min      45458.112527
    25%      46464.031184
    50%      46967.596354
    75%      47325.694987
    max      49243.765795
    dtype: float64



Let's compare this RMSE measured using cross-validation (the "validation error") with the RMSE measured on the training set (the "training error"):


```python
forest_reg.fit(housing, housing_labels)
housing_predictions = forest_reg.predict(housing)
forest_rmse = mean_squared_error(housing_labels, housing_predictions,
                                 squared=False)
forest_rmse
```




    17474.619286483998



The training error is much lower than the validation error, which usually means that the model has overfit the training set. Another possible explanation may be that there's a mismatch between the training data and the validation data, but it's not the case here, since both came from the same dataset that we shuffled and split in two parts.

# Fine-Tune Your Model

## Grid Search

**Warning:** the following cell may take a few minutes to run:


```python
from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)
```




    GridSearchCV(cv=3,
                 estimator=Pipeline(steps=[('preprocessing',
                                            ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                                         SimpleImputer(strategy='median')),
                                                                                        ('standardscaler',
                                                                                         StandardScaler())]),
                                                              transformers=[('bedrooms',
                                                                             Pipeline(steps=[('simpleimputer',
                                                                                              SimpleImputer(strategy='median')),
                                                                                             ('functiontransformer',
                                                                                              FunctionTransformer(feature_names_out=<f...
                                                                             <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                                           ('random_forest',
                                            RandomForestRegressor(random_state=42))]),
                 param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],
                              'random_forest__max_features': [4, 6, 8]},
                             {'preprocessing__geo__n_clusters': [10, 15],
                              'random_forest__max_features': [6, 8, 10]}],
                 scoring='neg_root_mean_squared_error')



You can get the full list of hyperparameters available for tuning by looking at `full_pipeline.get_params().keys()`:


```python
# extra code – shows part of the output of get_params().keys()
print(str(full_pipeline.get_params().keys())[:1000] + "...")
```

    dict_keys(['memory', 'steps', 'verbose', 'preprocessing', 'random_forest', 'preprocessing__n_jobs', 'preprocessing__remainder__memory', 'preprocessing__remainder__steps', 'preprocessing__remainder__verbose', 'preprocessing__remainder__simpleimputer', 'preprocessing__remainder__standardscaler', 'preprocessing__remainder__simpleimputer__add_indicator', 'preprocessing__remainder__simpleimputer__copy', 'preprocessing__remainder__simpleimputer__fill_value', 'preprocessing__remainder__simpleimputer__missing_values', 'preprocessing__remainder__simpleimputer__strategy', 'preprocessing__remainder__simpleimputer__verbose', 'preprocessing__remainder__standardscaler__copy', 'preprocessing__remainder__standardscaler__with_mean', 'preprocessing__remainder__standardscaler__with_std', 'preprocessing__remainder', 'preprocessing__sparse_threshold', 'preprocessing__transformer_weights', 'preprocessing__transformers', 'preprocessing__verbose', 'preprocessing__verbose_feature_names_out', 'preprocessing__be...


The best hyperparameter combination found:


```python
grid_search.best_params_
```




    {'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}




```python
grid_search.best_estimator_
```




    Pipeline(steps=[('preprocessing',
                     ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                  SimpleImputer(strategy='median')),
                                                                 ('standardscaler',
                                                                  StandardScaler())]),
                                       transformers=[('bedrooms',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='median')),
                                                                      ('functiontransformer',
                                                                       FunctionTransformer(feature_names_out=<function ratio_name at 0x1a5b6fd...
                                                      ClusterSimilarity(n_clusters=15,
                                                                        random_state=42),
                                                      ['latitude', 'longitude']),
                                                     ('cat',
                                                      Pipeline(steps=[('simpleimputer',
                                                                       SimpleImputer(strategy='most_frequent')),
                                                                      ('onehotencoder',
                                                                       OneHotEncoder(handle_unknown='ignore'))]),
                                                      <sklearn.compose._column_transformer.make_column_selector object at 0x1a5cdffd0>)])),
                    ('random_forest',
                     RandomForestRegressor(max_features=6, random_state=42))])



Let's look at the score of each hyperparameter combination tested during the grid search:


```python
cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)

# extra code – these few lines of code just make the DataFrame look nicer
cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", "split0_test_score",
                 "split1_test_score", "split2_test_score", "mean_test_score"]]
score_cols = ["split0", "split1", "split2", "mean_test_rmse"]
cv_res.columns = ["n_clusters", "max_features"] + score_cols
cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)

cv_res.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n_clusters</th>
      <th>max_features</th>
      <th>split0</th>
      <th>split1</th>
      <th>split2</th>
      <th>mean_test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>12</th>
      <td>15</td>
      <td>6</td>
      <td>43460</td>
      <td>43919</td>
      <td>44748</td>
      <td>44042</td>
    </tr>
    <tr>
      <th>13</th>
      <td>15</td>
      <td>8</td>
      <td>44132</td>
      <td>44075</td>
      <td>45010</td>
      <td>44406</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>10</td>
      <td>44374</td>
      <td>44286</td>
      <td>45316</td>
      <td>44659</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>6</td>
      <td>44683</td>
      <td>44655</td>
      <td>45657</td>
      <td>44999</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>6</td>
      <td>44683</td>
      <td>44655</td>
      <td>45657</td>
      <td>44999</td>
    </tr>
  </tbody>
</table>
</div>



## Randomized Search


```python
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV
```

Try 30 (`n_iter` × `cv`) random combinations of hyperparameters:

**Warning:** the following cell may take a few minutes to run:


```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(housing, housing_labels)
```




    RandomizedSearchCV(cv=3,
                       estimator=Pipeline(steps=[('preprocessing',
                                                  ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                                               SimpleImputer(strategy='median')),
                                                                                              ('standardscaler',
                                                                                               StandardScaler())]),
                                                                    transformers=[('bedrooms',
                                                                                   Pipeline(steps=[('simpleimputer',
                                                                                                    SimpleImputer(strategy='median')),
                                                                                                   ('functiontransformer',
                                                                                                    FunctionTransformer(feature_names_...
                                                                                   <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                                                 ('random_forest',
                                                  RandomForestRegressor(random_state=42))]),
                       param_distributions={'preprocessing__geo__n_clusters': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x1a4bfcb20>,
                                            'random_forest__max_features': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x1a57c7bb0>},
                       random_state=42, scoring='neg_root_mean_squared_error')




```python
# extra code – displays the random search results
cv_res = pd.DataFrame(rnd_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", "split0_test_score",
                 "split1_test_score", "split2_test_score", "mean_test_score"]]
cv_res.columns = ["n_clusters", "max_features"] + score_cols
cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)
cv_res.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>n_clusters</th>
      <th>max_features</th>
      <th>split0</th>
      <th>split1</th>
      <th>split2</th>
      <th>mean_test_rmse</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>45</td>
      <td>9</td>
      <td>41287</td>
      <td>42150</td>
      <td>42627</td>
      <td>42021</td>
    </tr>
    <tr>
      <th>8</th>
      <td>32</td>
      <td>7</td>
      <td>41690</td>
      <td>42542</td>
      <td>43224</td>
      <td>42485</td>
    </tr>
    <tr>
      <th>0</th>
      <td>41</td>
      <td>16</td>
      <td>42223</td>
      <td>42959</td>
      <td>43321</td>
      <td>42834</td>
    </tr>
    <tr>
      <th>5</th>
      <td>42</td>
      <td>4</td>
      <td>41818</td>
      <td>43094</td>
      <td>43817</td>
      <td>42910</td>
    </tr>
    <tr>
      <th>2</th>
      <td>23</td>
      <td>8</td>
      <td>42264</td>
      <td>42996</td>
      <td>43830</td>
      <td>43030</td>
    </tr>
  </tbody>
</table>
</div>



**Bonus section: how to choose the sampling distribution for a hyperparameter**

* `scipy.stats.randint(a, b+1)`: for hyperparameters with _discrete_ values that range from a to b, and all values in that range seem equally likely.
* `scipy.stats.uniform(a, b)`: this is very similar, but for _continuous_ hyperparameters.
* `scipy.stats.geom(1 / scale)`: for discrete values, when you want to sample roughly in a given scale. E.g., with scale=1000 most samples will be in this ballpark, but ~10% of all samples will be <100 and ~10% will be >2300.
* `scipy.stats.expon(scale)`: this is the continuous equivalent of `geom`. Just set `scale` to the most likely value.
* `scipy.stats.loguniform(a, b)`: when you have almost no idea what the optimal hyperparameter value's scale is. If you set a=0.01 and b=100, then you're just as likely to sample a value between 0.01 and 0.1 as a value between 10 and 100.


Here are plots of the probability mass functions (for discrete variables), and probability density functions (for continuous variables) for `randint()`, `uniform()`, `geom()` and `expon()`:


```python
# extra code – plots a few distributions you can use in randomized search

from scipy.stats import randint, uniform, geom, expon

xs1 = np.arange(0, 7 + 1)
randint_distrib = randint(0, 7 + 1).pmf(xs1)

xs2 = np.linspace(0, 7, 500)
uniform_distrib = uniform(0, 7).pdf(xs2)

xs3 = np.arange(0, 7 + 1)
geom_distrib = geom(0.5).pmf(xs3)

xs4 = np.linspace(0, 7, 500)
expon_distrib = expon(scale=1).pdf(xs4)

plt.figure(figsize=(12, 7))

plt.subplot(2, 2, 1)
plt.bar(xs1, randint_distrib, label="scipy.randint(0, 7 + 1)")
plt.ylabel("Probability")
plt.legend()
plt.axis([-1, 8, 0, 0.2])

plt.subplot(2, 2, 2)
plt.fill_between(xs2, uniform_distrib, label="scipy.uniform(0, 7)")
plt.ylabel("PDF")
plt.legend()
plt.axis([-1, 8, 0, 0.2])

plt.subplot(2, 2, 3)
plt.bar(xs3, geom_distrib, label="scipy.geom(0.5)")
plt.xlabel("Hyperparameter value")
plt.ylabel("Probability")
plt.legend()
plt.axis([0, 7, 0, 1])

plt.subplot(2, 2, 4)
plt.fill_between(xs4, expon_distrib, label="scipy.expon(scale=1)")
plt.xlabel("Hyperparameter value")
plt.ylabel("PDF")
plt.legend()
plt.axis([0, 7, 0, 1])

plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_202_0.png)
    


Here are the PDF for `expon()` and `loguniform()` (left column), as well as the PDF of log(X) (right column). The right column shows the distribution of hyperparameter _scales_. You can see that `expon()` favors hyperparameters with roughly the desired scale, with a longer tail towards the smaller scales. But `loguniform()` does not favor any scale, they are all equally likely:


```python
# extra code – shows the difference between expon and loguniform

from scipy.stats import loguniform

xs1 = np.linspace(0, 7, 500)
expon_distrib = expon(scale=1).pdf(xs1)

log_xs2 = np.linspace(-5, 3, 500)
log_expon_distrib = np.exp(log_xs2 - np.exp(log_xs2))

xs3 = np.linspace(0.001, 1000, 500)
loguniform_distrib = loguniform(0.001, 1000).pdf(xs3)

log_xs4 = np.linspace(np.log(0.001), np.log(1000), 500)
log_loguniform_distrib = uniform(np.log(0.001), np.log(1000)).pdf(log_xs4)

plt.figure(figsize=(12, 7))

plt.subplot(2, 2, 1)
plt.fill_between(xs1, expon_distrib,
                 label="scipy.expon(scale=1)")
plt.ylabel("PDF")
plt.legend()
plt.axis([0, 7, 0, 1])

plt.subplot(2, 2, 2)
plt.fill_between(log_xs2, log_expon_distrib,
                 label="log(X) with X ~ expon")
plt.legend()
plt.axis([-5, 3, 0, 1])

plt.subplot(2, 2, 3)
plt.fill_between(xs3, loguniform_distrib,
                 label="scipy.loguniform(0.001, 1000)")
plt.xlabel("Hyperparameter value")
plt.ylabel("PDF")
plt.legend()
plt.axis([0.001, 1000, 0, 0.005])

plt.subplot(2, 2, 4)
plt.fill_between(log_xs4, log_loguniform_distrib,
                 label="log(X) with X ~ loguniform")
plt.xlabel("Log of hyperparameter value")
plt.legend()
plt.axis([-8, 1, 0, 0.2])

plt.show()
```


    
![png](02_end_to_end_machine_learning_project_files/02_end_to_end_machine_learning_project_204_0.png)
    


## Analyze the Best Models and Their Errors


```python
final_model = rnd_search.best_estimator_  # includes preprocessing
feature_importances = final_model["random_forest"].feature_importances_
feature_importances.round(2)
```




    array([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, 0.04, 0.01, 0.  ,
           0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.01, 0.01, 0.01, 0.  , 0.01,
           0.01, 0.01, 0.01, 0.01, 0.  , 0.  , 0.02, 0.01, 0.01, 0.01, 0.02,
           0.01, 0.  , 0.02, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01,
           0.01, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.01, 0.  , 0.07,
           0.  , 0.  , 0.  , 0.01])




```python
sorted(zip(feature_importances,
           final_model["preprocessing"].get_feature_names_out()),
           reverse=True)
```




    [(0.18694559869103852, 'log__median_income'),
     (0.0748194905715524, 'cat__ocean_proximity_INLAND'),
     (0.06926417748515576, 'bedrooms__ratio'),
     (0.05446998753775219, 'rooms_per_house__ratio'),
     (0.05262301809680712, 'people_per_house__ratio'),
     (0.03819415873915732, 'geo__Cluster 0 similarity'),
     (0.02879263999929514, 'geo__Cluster 28 similarity'),
     (0.023530192521380392, 'geo__Cluster 24 similarity'),
     (0.020544786346378206, 'geo__Cluster 27 similarity'),
     (0.019873052631077512, 'geo__Cluster 43 similarity'),
     (0.018597511022930273, 'geo__Cluster 34 similarity'),
     (0.017409085415656868, 'geo__Cluster 37 similarity'),
     (0.015546519677632162, 'geo__Cluster 20 similarity'),
     (0.014230331127504292, 'geo__Cluster 17 similarity'),
     (0.0141032216204026, 'geo__Cluster 39 similarity'),
     (0.014065768027447325, 'geo__Cluster 9 similarity'),
     (0.01354220782825315, 'geo__Cluster 4 similarity'),
     (0.01348963625822907, 'geo__Cluster 3 similarity'),
     (0.01338319626383868, 'geo__Cluster 38 similarity'),
     (0.012240533790212824, 'geo__Cluster 31 similarity'),
     (0.012089046542256785, 'geo__Cluster 7 similarity'),
     (0.01152326329703204, 'geo__Cluster 23 similarity'),
     (0.011397459905603558, 'geo__Cluster 40 similarity'),
     (0.011282340924816439, 'geo__Cluster 36 similarity'),
     (0.01104139770781063, 'remainder__housing_median_age'),
     (0.010671123191312802, 'geo__Cluster 44 similarity'),
     (0.010296376177202627, 'geo__Cluster 5 similarity'),
     (0.010184798445004483, 'geo__Cluster 42 similarity'),
     (0.010121853542225083, 'geo__Cluster 11 similarity'),
     (0.009795219101117579, 'geo__Cluster 35 similarity'),
     (0.00952581084310724, 'geo__Cluster 10 similarity'),
     (0.009433209165984823, 'geo__Cluster 13 similarity'),
     (0.00915075361116215, 'geo__Cluster 1 similarity'),
     (0.009021485619463173, 'geo__Cluster 30 similarity'),
     (0.00894936224917583, 'geo__Cluster 41 similarity'),
     (0.008901832702357514, 'geo__Cluster 25 similarity'),
     (0.008897504713401587, 'geo__Cluster 29 similarity'),
     (0.0086846298524955, 'geo__Cluster 21 similarity'),
     (0.008061104590483955, 'geo__Cluster 15 similarity'),
     (0.00786048176566994, 'geo__Cluster 16 similarity'),
     (0.007793633130749198, 'geo__Cluster 22 similarity'),
     (0.007501766442066527, 'log__total_rooms'),
     (0.0072024111938241275, 'geo__Cluster 32 similarity'),
     (0.006947156598995616, 'log__population'),
     (0.006800076770899128, 'log__households'),
     (0.006736105364684462, 'log__total_bedrooms'),
     (0.006315268213499131, 'geo__Cluster 33 similarity'),
     (0.005796398579893261, 'geo__Cluster 14 similarity'),
     (0.005234954623294958, 'geo__Cluster 6 similarity'),
     (0.0045514083468621595, 'geo__Cluster 12 similarity'),
     (0.004546042080216035, 'geo__Cluster 18 similarity'),
     (0.004314514641115755, 'geo__Cluster 2 similarity'),
     (0.003953528110719969, 'geo__Cluster 19 similarity'),
     (0.003297404747742136, 'geo__Cluster 26 similarity'),
     (0.00289453474290887, 'cat__ocean_proximity_<1H OCEAN'),
     (0.0016978863168109126, 'cat__ocean_proximity_NEAR OCEAN'),
     (0.0016391131530559377, 'geo__Cluster 8 similarity'),
     (0.00015061247730531558, 'cat__ocean_proximity_NEAR BAY'),
     (7.301686597099842e-05, 'cat__ocean_proximity_ISLAND')]



## Evaluate Your System on the Test Set


```python
X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)
```

    41424.40026462184


We can compute a 95% confidence interval for the test RMSE:


```python
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))
```




    array([39275.40861216, 43467.27680583])



We could compute the interval manually like this:


```python
# extra code – shows how to compute a confidence interval for the RMSE
m = len(squared_errors)
mean = squared_errors.mean()
tscore = stats.t.ppf((1 + confidence) / 2, df=m - 1)
tmargin = tscore * squared_errors.std(ddof=1) / np.sqrt(m)
np.sqrt(mean - tmargin), np.sqrt(mean + tmargin)
```




    (39275.40861216077, 43467.2768058342)



Alternatively, we could use a z-score rather than a t-score. Since the test set is not too small, it won't make a big difference:


```python
# extra code – computes a confidence interval again using a z-score
zscore = stats.norm.ppf((1 + confidence) / 2)
zmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)
np.sqrt(mean - zmargin), np.sqrt(mean + zmargin)
```




    (39276.05610140007, 43466.691749969636)



## Model persistence using joblib

Save the final model:


```python
import joblib

joblib.dump(final_model, "my_california_housing_model.pkl")
```




    ['my_california_housing_model.pkl']



Now you can deploy this model to production. For example, the following code could be a script that would run in production:


```python
import joblib

# extra code – excluded for conciseness
from sklearn.cluster import KMeans
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics.pairwise import rbf_kernel

def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

#class ClusterSimilarity(BaseEstimator, TransformerMixin):
#    [...]

final_model_reloaded = joblib.load("my_california_housing_model.pkl")

new_data = housing.iloc[:5]  # pretend these are new districts
predictions = final_model_reloaded.predict(new_data)
```


```python
predictions
```




    array([442737.15, 457566.06, 105965.  ,  98462.  , 332992.01])



You could use pickle instead, but joblib is more efficient.

# Exercise solutions

## 1.

Exercise: _Try a Support Vector Machine regressor (`sklearn.svm.SVR`) with various hyperparameters, such as `kernel="linear"` (with various values for the `C` hyperparameter) or `kernel="rbf"` (with various values for the `C` and `gamma` hyperparameters). Note that SVMs don't scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don't worry about what the hyperparameters mean for now (see the SVM notebook if you're interested). How does the best `SVR` predictor perform?_


```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR

param_grid = [
        {'svr__kernel': ['linear'], 'svr__C': [10., 30., 100., 300., 1000.,
                                               3000., 10000., 30000.0]},
        {'svr__kernel': ['rbf'], 'svr__C': [1.0, 3.0, 10., 30., 100., 300.,
                                            1000.0],
         'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
    ]

svr_pipeline = Pipeline([("preprocessing", preprocessing), ("svr", SVR())])
grid_search = GridSearchCV(svr_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])
```




    GridSearchCV(cv=3,
                 estimator=Pipeline(steps=[('preprocessing',
                                            ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                                         SimpleImputer(strategy='median')),
                                                                                        ('standardscaler',
                                                                                         StandardScaler())]),
                                                              transformers=[('bedrooms',
                                                                             Pipeline(steps=[('simpleimputer',
                                                                                              SimpleImputer(strategy='median')),
                                                                                             ('functiontransformer',
                                                                                              FunctionTransformer(feature_names_out=<f...
                                                                             <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                                           ('svr', SVR())]),
                 param_grid=[{'svr__C': [10.0, 30.0, 100.0, 300.0, 1000.0, 3000.0,
                                         10000.0, 30000.0],
                              'svr__kernel': ['linear']},
                             {'svr__C': [1.0, 3.0, 10.0, 30.0, 100.0, 300.0,
                                         1000.0],
                              'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0],
                              'svr__kernel': ['rbf']}],
                 scoring='neg_root_mean_squared_error')



The best model achieves the following score (evaluated using 3-fold cross validation):


```python
svr_grid_search_rmse = -grid_search.best_score_
svr_grid_search_rmse
```




    69814.13889867254



That's much worse than the `RandomForestRegressor` (but to be fair, we trained the model on much less data). Let's check the best hyperparameters found:


```python
grid_search.best_params_
```




    {'svr__C': 10000.0, 'svr__kernel': 'linear'}



The linear kernel seems better than the RBF kernel. Notice that the value of `C` is the maximum tested value. When this happens you definitely want to launch the grid search again with higher values for `C` (removing the smallest values), because it is likely that higher values of `C` will be better.

## 2.

Exercise: _Try replacing the `GridSearchCV` with a `RandomizedSearchCV`._

**Warning:** the following cell will take several minutes to run. You can specify `verbose=2` when creating the `RandomizedSearchCV` if you want to see the training details.


```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import expon, loguniform

# see https://docs.scipy.org/doc/scipy/reference/stats.html
# for `expon()` and `loguniform()` documentation and more probability distribution functions.

# Note: gamma is ignored when kernel is "linear"
param_distribs = {
        'svr__kernel': ['linear', 'rbf'],
        'svr__C': loguniform(20, 200_000),
        'svr__gamma': expon(scale=1.0),
    }

rnd_search = RandomizedSearchCV(svr_pipeline,
                                param_distributions=param_distribs,
                                n_iter=50, cv=3,
                                scoring='neg_root_mean_squared_error',
                                random_state=42)
rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])
```




    RandomizedSearchCV(cv=3,
                       estimator=Pipeline(steps=[('preprocessing',
                                                  ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',
                                                                                               SimpleImputer(strategy='median')),
                                                                                              ('standardscaler',
                                                                                               StandardScaler())]),
                                                                    transformers=[('bedrooms',
                                                                                   Pipeline(steps=[('simpleimputer',
                                                                                                    SimpleImputer(strategy='median')),
                                                                                                   ('functiontransformer',
                                                                                                    FunctionTransformer(feature_names_...
                                                                                   <sklearn.compose._column_transformer.make_column_selector object at 0x1a57e3a00>)])),
                                                 ('svr', SVR())]),
                       n_iter=50,
                       param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a5d4c3a0>,
                                            'svr__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a5d9ca00>,
                                            'svr__kernel': ['linear', 'rbf']},
                       random_state=42, scoring='neg_root_mean_squared_error')



The best model achieves the following score (evaluated using 3-fold cross validation):


```python
svr_rnd_search_rmse = -rnd_search.best_score_
svr_rnd_search_rmse
```




    55853.88100300133



Now that's really much better, but still far from the `RandomForestRegressor`'s performance. Let's check the best hyperparameters found:


```python
rnd_search.best_params_
```




    {'svr__C': 157055.10989448498,
     'svr__gamma': 0.26497040005002437,
     'svr__kernel': 'rbf'}



This time the search found a good set of hyperparameters for the RBF kernel. Randomized search tends to find better hyperparameters than grid search in the same amount of time.

Note that we used the `expon()` distribution for `gamma`, with a scale of 1, so `RandomSearch` mostly searched for values roughly of that scale: about 80% of the samples were between 0.1 and 2.3 (roughly 10% were smaller and 10% were larger):


```python
np.random.seed(42)

s = expon(scale=1).rvs(100_000)  # get 100,000 samples
((s > 0.105) & (s < 2.29)).sum() / 100_000
```




    0.80066



We used the `loguniform()` distribution for `C`, meaning we did not have a clue what the optimal scale of `C` was before running the random search. It explored the range from 20 to 200 just as much as the range from 2,000 to 20,000 or from 20,000 to 200,000.

## 3.

Exercise: _Try adding a `SelectFromModel` transformer in the preparation pipeline to select only the most important attributes._

Let's create a new pipeline that runs the previously defined preparation pipeline, and adds a `SelectFromModel` transformer based on a `RandomForestRegressor` before the final regressor:


```python
from sklearn.feature_selection import SelectFromModel

selector_pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),
                                 threshold=0.005)),  # min feature importance
    ('svr', SVR(C=rnd_search.best_params_["svr__C"],
                gamma=rnd_search.best_params_["svr__gamma"],
                kernel=rnd_search.best_params_["svr__kernel"])),
])
```


```python
selector_rmses = -cross_val_score(selector_pipeline,
                                  housing.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  scoring="neg_root_mean_squared_error",
                                  cv=3)
pd.Series(selector_rmses).describe()
```




    count        3.000000
    mean     56211.362086
    std       1922.002802
    min      54150.008629
    25%      55339.929909
    50%      56529.851189
    75%      57242.038815
    max      57954.226441
    dtype: float64



Oh well, feature selection does not seem to help. But maybe that's just because the threshold we used was not optimal. Perhaps try tuning it using random search or grid search?

## 4.

Exercise: _Try creating a custom transformer that trains a k-Nearest Neighbors regressor (`sklearn.neighbors.KNeighborsRegressor`) in its `fit()` method, and outputs the model's predictions in its `transform()` method. Then add this feature to the preprocessing pipeline, using latitude and longitude as the inputs to this transformer. This will add a feature in the model that corresponds to the housing median price of the nearest districts._

Rather than restrict ourselves to k-Nearest Neighbors regressors, let's create a transformer that accepts any regressor. For this, we can extend the `MetaEstimatorMixin` and have a required `estimator` argument in the constructor. The `fit()` method must work on a clone of this estimator, and it must also save `feature_names_in_`. The `MetaEstimatorMixin` will ensure that `estimator` is listed as a required parameters, and it will update `get_params()` and `set_params()` to make the estimator's hyperparameters available for tuning. Lastly, we create a `get_feature_names_out()` method: the output column name is the ...


```python
from sklearn.neighbors import KNeighborsRegressor
from sklearn.base import MetaEstimatorMixin, clone

class FeatureFromRegressor(MetaEstimatorMixin, BaseEstimator, TransformerMixin):
    def __init__(self, estimator):
        self.estimator = estimator

    def fit(self, X, y=None):
        estimator_ = clone(self.estimator)
        estimator_.fit(X, y)
        self.estimator_ = estimator_
        self.n_features_in_ = self.estimator_.n_features_in_
        if hasattr(self.estimator, "feature_names_in_"):
            self.feature_names_in_ = self.estimator.feature_names_in_
        return self  # always return self!
    
    def transform(self, X):
        check_is_fitted(self)
        predictions = self.estimator_.predict(X)
        if predictions.ndim == 1:
            predictions = predictions.reshape(-1, 1)
        return predictions

    def get_feature_names_out(self, names=None):
        check_is_fitted(self)
        n_outputs = getattr(self.estimator_, "n_outputs_", 1)
        estimator_class_name = self.estimator_.__class__.__name__
        estimator_short_name = estimator_class_name.lower().replace("_", "")
        return [f"{estimator_short_name}_prediction_{i}"
                for i in range(n_outputs)]
```

Let's ensure it complies to Scikit-Learn's API:


```python
from sklearn.utils.estimator_checks import check_estimator

check_estimator(FeatureFromRegressor(KNeighborsRegressor()))
```

Good! Now let's test it:


```python
knn_reg = KNeighborsRegressor(n_neighbors=3, weights="distance")
knn_transformer = FeatureFromRegressor(knn_reg)
geo_features = housing[["latitude", "longitude"]]
knn_transformer.fit_transform(geo_features, housing_labels)
```




    array([[456667.        ],
           [435250.        ],
           [105100.        ],
           ...,
           [148800.        ],
           [500001.        ],
           [234333.33333333]])



And what does its output feature name look like?


```python
knn_transformer.get_feature_names_out()
```




    ['kneighborsregressor_prediction_0']



Okay, now let's include this transformer in our preprocessing pipeline:


```python
from sklearn.base import clone

transformers = [(name, clone(transformer), columns)
                for name, transformer, columns in preprocessing.transformers]
geo_index = [name for name, _, _ in transformers].index("geo")
transformers[geo_index] = ("geo", knn_transformer, ["latitude", "longitude"])

new_geo_preprocessing = ColumnTransformer(transformers)
```


```python
new_geo_pipeline = Pipeline([
    ('preprocessing', new_geo_preprocessing),
    ('svr', SVR(C=rnd_search.best_params_["svr__C"],
                gamma=rnd_search.best_params_["svr__gamma"],
                kernel=rnd_search.best_params_["svr__kernel"])),
])
```


```python
new_pipe_rmses = -cross_val_score(new_geo_pipeline,
                                  housing.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  scoring="neg_root_mean_squared_error",
                                  cv=3)
pd.Series(new_pipe_rmses).describe()
```




    count         3.000000
    mean     104992.095758
    std        3112.486560
    min      101550.880533
    25%      103682.876337
    50%      105814.872141
    75%      106712.703370
    max      107610.534600
    dtype: float64



Yikes, that's terrible! Apparently the cluster similarity features were much better. But perhaps we should tune the `KNeighborsRegressor`'s hyperparameters? That's what the next exercise is about.

## 5.

Exercise: _Automatically explore some preparation options using `RandomSearchCV`._


```python
param_distribs = {
    "preprocessing__geo__estimator__n_neighbors": range(1, 30),
    "preprocessing__geo__estimator__weights": ["distance", "uniform"],
    "svr__C": loguniform(20, 200_000),
    "svr__gamma": expon(scale=1.0),
}

new_geo_rnd_search = RandomizedSearchCV(new_geo_pipeline,
                                        param_distributions=param_distribs,
                                        n_iter=50,
                                        cv=3,
                                        scoring='neg_root_mean_squared_error',
                                        random_state=42)
new_geo_rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])
```




    RandomizedSearchCV(cv=3,
                       estimator=Pipeline(steps=[('preprocessing',
                                                  ColumnTransformer(transformers=[('bedrooms',
                                                                                   Pipeline(steps=[('simpleimputer',
                                                                                                    SimpleImputer(strategy='median')),
                                                                                                   ('functiontransformer',
                                                                                                    FunctionTransformer(feature_names_out=<function ratio_name at 0x1a5b6fd90>,
                                                                                                                        func=<function column_ratio at 0x1a5695bd0>)),
                                                                                                   ('standardscaler',
                                                                                                    StandardScaler()...
                       param_distributions={'preprocessing__geo__estimator__n_neighbors': range(1, 30),
                                            'preprocessing__geo__estimator__weights': ['distance',
                                                                                       'uniform'],
                                            'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a63fda80>,
                                            'svr__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x1a63fe410>},
                       random_state=42, scoring='neg_root_mean_squared_error')




```python
new_geo_rnd_search_rmse = -new_geo_rnd_search.best_score_
new_geo_rnd_search_rmse
```




    106775.63787128967



Oh well... at least we tried! It looks like the cluster similarity features are definitely better than the KNN feature. But perhaps you could try having both? And maybe training on the full training set would help as well.

## 6.

Exercise: _Try to implement the `StandardScalerClone` class again from scratch, then add support for the `inverse_transform()` method: executing `scaler.inverse_transform(scaler.fit_transform(X))` should return an array very close to `X`. Then add support for feature names: set `feature_names_in_` in the `fit()` method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the `get_feature_names_out()` method: it should have one optional `input_features=None` argument. If passed, the method should check that its length matches `n_features_in_`, and it should match `feature_names_in_` if it is defined, then `input_features` should be returned. If `input_features` is `None`, then the method should return `feature_names_in_` if it is defined or `np.array(["x0", "x1", ...])` with length `n_features_in_` otherwise._


```python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # no *args or **kwargs!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # y is required even though we don't use it
        X_orig = X
        X = check_array(X)  # checks that X is an array with finite float values
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
        if hasattr(X_orig, "columns"):
            self.feature_names_in_ = np.array(X_orig.columns, dtype=object)
        return self  # always return self!

    def transform(self, X):
        check_is_fitted(self)  # looks for learned attributes (with trailing _)
        X = check_array(X)
        if self.n_features_in_ != X.shape[1]:
            raise ValueError("Unexpected number of features")
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
    
    def inverse_transform(self, X):
        check_is_fitted(self)
        X = check_array(X)
        if self.n_features_in_ != X.shape[1]:
            raise ValueError("Unexpected number of features")
        X = X * self.scale_
        return X + self.mean_ if self.with_mean else X
    
    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            return getattr(self, "feature_names_in_",
                           [f"x{i}" for i in range(self.n_features_in_)])
        else:
            if len(input_features) != self.n_features_in_:
                raise ValueError("Invalid number of features")
            if hasattr(self, "feature_names_in_") and not np.all(
                self.feature_names_in_ == input_features
            ):
                raise ValueError("input_features ≠ feature_names_in_")
            return input_features
```

Let's test our custom transformer:


```python
from sklearn.utils.estimator_checks import check_estimator
 
check_estimator(StandardScalerClone())
```

No errors, that's a great start, we respect the Scikit-Learn API.

Now let's ensure the transformation works as expected:


```python
np.random.seed(42)
X = np.random.rand(1000, 3)

scaler = StandardScalerClone()
X_scaled = scaler.fit_transform(X)

assert np.allclose(X_scaled, (X - X.mean(axis=0)) / X.std(axis=0))
```

How about setting `with_mean=False`?


```python
scaler = StandardScalerClone(with_mean=False)
X_scaled_uncentered = scaler.fit_transform(X)

assert np.allclose(X_scaled_uncentered, X / X.std(axis=0))
```

And does the inverse work?


```python
scaler = StandardScalerClone()
X_back = scaler.inverse_transform(scaler.fit_transform(X))

assert np.allclose(X, X_back)
```

How about the feature names out?


```python
assert np.all(scaler.get_feature_names_out() == ["x0", "x1", "x2"])
assert np.all(scaler.get_feature_names_out(["a", "b", "c"]) == ["a", "b", "c"])
```

And if we fit a DataFrame, are the feature in and out ok?


```python
df = pd.DataFrame({"a": np.random.rand(100), "b": np.random.rand(100)})
scaler = StandardScalerClone()
X_scaled = scaler.fit_transform(df)

assert np.all(scaler.feature_names_in_ == ["a", "b"])
assert np.all(scaler.get_feature_names_out() == ["a", "b"])
```

All good! That's all for today! 😀

Congratulations! You already know quite a lot about Machine Learning. :)
